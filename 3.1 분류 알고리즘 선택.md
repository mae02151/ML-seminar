## 3.1 분류 알고리즘 선택

##### 	알고리즘은 각자의 특징이 있고 특정 문제에 알맞은 분류 알고리즘을 선택하기 위해서는 연습이 필요하다.



#### 알고리즘을 선택하는 기준

---



>- ###### 데이터 크기
>
>- ###### 가용 연산(계산) 시간
>
>- ###### 정확성
>
>- ###### 사용 편의성
>
>  [분류 알고리즘 가이드라인](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)



#### 머신 러닝 알고리즘 종류

---

#### 1. 선형 회귀(Linear regression)와 로지스틱 회귀(Logistic regression)	

![](https://blogs.sas.com/content/saskorea/files/2017/08/logisticregresion-300x225.png)

##### 장점

- 학습 속도가 빠르고 예측도 빠르다.
- 큰 데이터 셋과 희소한 데이터셋에도 잘 작동한다.
- 수식을 이해하기 비교적 쉬움



##### 단점

- 오버피팅 가능성이 있다.



#### 2. k-NN classifier & regressor



![](https://blogs.sas.com/content/saskorea/files/2019/03/%EC%B9%98%ED%8A%B816.png)

##### 장점

- 이해하기 쉬운 모델.
- 많은 조정 없이 좋은 성능을 발휘.
- 데이터셋에서 가장 가까이 이는 이웃을 보고 결정하는 방식.

> [k-NN 설명 잘되있는 곳](https://gdpresent.blog.me/221690401968)

##### 단점

- 훈련 세트가 매우크면 예측이 느려진다.
- 많은 특성을 처리하는 능력이 부족해 현업에서 잘 쓰이지 않음
- 학습된 데이터의 범위를 벗어나는 데이터는 분석이 불가능.



#### 3. Deep learning(딥 러닝)

![](http://www.bloter.net/wp-content/uploads/2014/07/DeepNetwork.jpg)

##### 장점

- 데이터를 고수준 패턴을 복합적인 다계층 네트워크 모델링하는 법.
- 컴퓨터 비전과 자동 음성 인식 분야에서 최고의 성능.



##### 단점

- 수집된 데이터가 충분히 촘촘하지 않으면 잘 작동하지 않음
- 학습 실시간성 문제가 있음.
- 학습에 데이터가 매우 많이 필요.





### 3.2 사이킷런:퍼셉트론 훈련

___

```python
from sklearn import datasets
import numpy as np

iris = datasets.load_iris()		#iris 데이터 불러오기
X = iris.data[:, [2, 3]]		#꽃잎 길이와 넓이 값
y = iris.target 				

print('클래스 레이블:', np.unique(y))
```

> ###### > 클래스 레이블: [0, 1 , 2 ]
>
> ###### 여기서 iris.target은 iris데이터의 레이블의 종류를 출력한다.
>
> ######  np.unique() 함수는 iris의 레이블 종류를 중복 없이 정수화 시켜 출력해준다.



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1, stratify=y)
```

> ##### train_test_split은 test데이터와 train데이터를 섞어서 설정한 비율로 분리해 주는 함수이다.
>
> test_size는 테스트 데이터의 비율을 의미하고 0.0~1.0 사이의 값을 써야한다(0.3은 30%를 뜻한다.) .  random_state=1로 하는 이유는 랜덤으로 되는 값을 고정시키기 위해서이다. stratify = y로 입력하면 테스트 데이터와 학습 데이터의 비율을 처음 설정한 test:30, train:70인 상태로 유지할 수 있다.
>
> [train_test_split 참조](https://roboreport.co.kr/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-%ED%9B%88%EB%A0%A8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B5%AC%EC%84%B1-%EB%B0%A9%EB%B2%95/)



```
print('y의 레이블 카운트:', np.bincount(y))
print('y_train의 레이블 카운트:', np.bincount(y_train))
print('y_test의 레이블 카운트:', np.bincount(y_test))
```

> ###### > y의 레이블 카운트: [50 50 50]
>
> ###### > y_train의 레이블 카운트: [36 32 37] 
>
> ###### > y_test의 레이블 카운트: [14 18 13]
>
> np.bincount 함수는 데이터에서 같은 특성의 수를 세는 함수이다. 아래의 예로 더 자세히 설명해 보겠다.
>
> y = [1, 2, 2, 3, 4 , 5 ,5] -> np.bincount(y)를 하면
>
> ###### > [0, 1, 2, 1, 1, 2]  가 출력된다.
>
> 왜 이런 결과가 나오나면 출력 결과의 순서는 index 번호 순서이다. 즉 y 행렬에서 0은 없기에 결과 행렬 첫번째의 값이 0이고 1은 하나가 있기 때문에 1이 출력이 된다. 그리고 행렬에서 2값은 2개가 있어서 결과값에서 2번째 자리에서는 2가 출력 된것을 볼 수 있다. 



```python
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
```

> ###### 특성을 표준화 시키기 위해 sklearn에서의 StandardScaler를 사용한다.(StandardScaler는 분포의 평균을 0 표준편차가 1이 되록 만든다)
>
> ![](https://t1.daumcdn.net/cfile/tistory/997D23455C63973C2D)
>
> ​								스케일링한 값들 이미지 출저(https://homeproject.tistory.com/3)
>
> 이것을 하는 이유는 머신러닝의 경우 정규분포에 가까운 경우 더 나은 성능을 발휘하거나 더 빨리 수렴하기 때문이다.
>
> sc.fit은 데이터 변환을 학습하고, transform은 실제 데이터의 스케일을 조정한다.
>
> 이때, <u>fit 메서드는 학습용 데이터에만 적용해야한다.</u> 그후 transform 메서드에 학습용 데이터와 테스트 데이터를 적용한다.
>
> [StandardScaler 설명](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)
>
> [스케일링관한 설명 사이트](https://homeproject.tistory.com/3)



```python
from sklearn.linear_model import Perceptron

ppn = Perceptron(max_iter=40, eta0=0.1, tol=1e-3, random_state=1)
ppn.fit(X_train_std, y_train)
```

> sklearn의 linear_model에 있는 perceptron을 불러와 ppn에 perceptron 객체를 생성한다.
>
> max_iter는 훈련세트를 반복할 에포크 횟수를 말한다. eta는 학습률을 의미한다. tol은 반복에서 수렴할 오차를 말한다(loss > previous_loss - tol). 그리고 ppn 객체에 앞에서 스케일링한 데이터들을 넣는다.



```
from sklearn.metrics import accuracy_score

print('정확도: %.2f' % accuracy_score(y_test, y_pred))
```

> y_test 와 y_pred 값을 비교하여 정확도를 측정한다.
>
> ex) 45개의 데이터중 오차 1 오차: 1/45*100 = 2%
>
> 정확도:  1- 0.02 = 0.98 = 98%



```python
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt


def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):

    # 마커와 컬러맵을 설정합니다.
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # 결정 경계를 그립니다.
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1	# xx1:258, xx2:265,
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) # (68370,)
    Z = Z.reshape(xx1.shape) 	#258, 265
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='black')

    # 테스트 샘플을 부각하여 그립니다.
    if test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]

        plt.scatter(X_test[:, 0],
                    X_test[:, 1],
                    c='',
                    edgecolor='black',
                    alpha=1.0,
                    linewidth=1,
                    marker='o',
                    s=100, 
                    label='test set')
```

> x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 이 부분은 표의 경계를 그리기 위해 설정해준 값들이다.
>
> xx1, xx2 = np.meshgrid() x, y 값의 grid point를 생성할 수 있게 해주는 함수이다. 여기서는 contouf에 변수들을 사용하기 위해서는 2차원인 변수를 사용하기 때문에 사용한다(countourf를 사용하기 위해서는 meshgrid를 사용해야한다.).
>
> contourf는 여기서 경계를 색칠하는 함수이다. x, y는 경계의 값이고 z는 레이블 값이 들어간다. 그리고 값들의 shape를 모두 같게 해주어야한다.
>
> for idx, cl in enumerate(np.unique(y)) 이 구간은 그래프에 값을 부호로 표시해 주기위해 사용했다. 
>
> x=X[y == cl, 0]은 사용한 이유는 각 레이블마다 숫자와 기호를 다르게 그려서 분류할 수 있게 하기 위해 사용되었다. alpha는 분류 모양의 투명도를 의미한다.
>
> [meshgrid 설명](https://datascienceschool.net/view-notebook/17608f897087478bbeac096438c716f6/)



```python
        plt.scatter(X_test[:, 0],
                    X_test[:, 1],
                    c='',
                    edgecolor='black',
                    alpha=1.0,
                    linewidth=1,
                    marker='o',
                    s=100, 				
                    label='test set')
```

> 여기서는 테스트 데이터를 따로 그려주기 위해서 사용되었다.



```python
X_combined_std = np.vstack((X_train_std, X_test_std))	#150,2
y_combined = np.hstack((y_train, y_test))				#150,
print(y_combined)
plot_decision_regions(X=X_combined_std, y=y_combined,
                      classifier=ppn, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()
```

> np.vstack은 서로 다른 행렬을 합쳐줄 때 쓰는 함수이다. 
>
> ex) A = [[1, 2],[2,3]] B = [[4,5]]
>
> np.vstack(A,B)의 결과는 
>
> ###### > [[1,2]
>
> [2,3]
>
> [4,5]] 이다.
>
> np.vstack()과 np.hstack()의 기능은 동일한데 차이점은 np.vstack은 밑으로 더해지며 합쳐지는식이고 np.hstack()은 옆으로 더해진다.
>
> tight_layout()은 자동으로 표의 간격을 맞춰준다.
>
> [np.vstack 설명](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html)