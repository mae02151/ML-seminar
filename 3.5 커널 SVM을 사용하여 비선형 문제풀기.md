# 3.5 커널 SVM을 사용하여 비선형 문제풀기



#### 선형 분류 모델의 한계

![](https://tensorflowkorea.files.wordpress.com/2017/06/2-36.png?w=625)

##### 																				예시로 사용되는 데이터셋



![](https://tensorflowkorea.files.wordpress.com/2017/06/2-37.png?w=625)

##### 																		선형 SVM으로 만들어진 결정 경계

> 선형 모델은 XOR 데이터셋을 구분할 수 없다. 
>
> [이미지 출저](https://tensorflow.blog/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2-3-7-%EC%BB%A4%EB%84%90-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0/)



위 데이터를 보면 선형분류로는 학습하는것이 불가능 하다. 이러한 문제를 해결하기 위해 저차원 데이터를 고차원 공간으로 매핑하여 분류한다.



![](http://i.imgur.com/4i3ILsZ.png)

> 원공간(INPUT Space)의 데이터를 선형 분류가 가능하도록 고차원 공간(Feature Space)로 매핑한 뒤 두 범주를 분류하는 초평면을 찾는 방법.



저차원 데이터를 고차원 데이터로 바꿔서 분류하는 것은 매우 효과적이지만 단점도 있다. 바로 **연산비용**이 매우 커진다는 점이다.  왜냐하면 모든 관측치에 대해 고차원으로 매핑한 뒤 이를 다시 **내적(inner product)** 해야하기 때문이다. 이러한 문제를 해결하기 위해 매핑과 내적을 한방에 할수 있는 **커널함수(Kernel function)** 를 사용한다.



## Kernel function

> 선형 분류가 어려운 저차원 데이터를 고차원 공간으로 매핑하는 함수.
> 두 폰인 사이 점곱을 계산하는 데 드는 높은 비용을 절감하기 위해 사용한다.

![](https://user-images.githubusercontent.com/37826820/69931035-2356c700-1509-11ea-9819-1d890ed14358.PNG)

이러한 과정으로 **커널 K**의 값은 스칼라가 된다.



### 커널 사용의 장점

커널을 사용하면 basic 함수를 하나씩 정의하는 연산을 줄일 수 있어 계산량이 줄어든다. 예를 들어 다음과 같은 식으로 표현해 보겠다.

![](https://user-images.githubusercontent.com/37826820/69931031-22be3080-1509-11ea-9b94-4933b1959a00.PNG)

위 식에서 커널방법을 쓰지 않고 

![](https://user-images.githubusercontent.com/37826820/69931035-2356c700-1509-11ea-9819-1d890ed14358.PNG)

위의 결과를 구하기 위해서는

![](https://user-images.githubusercontent.com/37826820/69931029-22be3080-1509-11ea-90e8-7c8fac540822.PNG)

계산을 총 합하면 11번 하게 된다. 하지만 이 식에서 커널 방식을 쓰게 되면 연산이 확연하게 줄어드는것을 볼 수 있다.

![](https://user-images.githubusercontent.com/37826820/69931034-2356c700-1509-11ea-9ccf-e9109b7407e6.PNG)

**Kernel** 방식을 사용하면 위 식을 계산하는데 곱셈이 2번 밖에 필요하지 않아 연산량이 크게 줄어든 것을 볼  수 있다.



- ##### Kernel의 종류(참고)

![](https://user-images.githubusercontent.com/37826820/69931033-2356c700-1509-11ea-8b2f-f4bdb9b84054.PNG)

여기서 가장 많이 쓰이는 것은 **Gaussian Kernel(RBF Kernel이라고도 부름)**이다.

여기서 분모 1로 생략.

![](https://user-images.githubusercontent.com/37826820/69931037-23ef5d80-1509-11ea-8caa-6ce904778c08.PNG)

테일러급수 정리에 의해 

![](https://user-images.githubusercontent.com/37826820/69931036-2356c700-1509-11ea-8869-b77f8ae73eb3.PNG)

는 다음과 같이 쓸 수 있다.

![](https://user-images.githubusercontent.com/37826820/69931032-22be3080-1509-11ea-9da9-a38462c3241d.PNG)

Gaussian Kernel은 input space가 몇 차원이든 무한대 차원의 Feature Space로 매핑한다는 얘기이다.(흠)

> 참고 자료들
>
> [Kernel svm 참조](https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/)
>
> [Kernel 참조(쉽게 설명 잘 해주심) ](https://datascienceschool.net/view-notebook/69278a5de79449019ad1f51a614ef87c/)





## **커널 SVM을 사용하여 비선형 문제 풀기**

```python
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(1)			# 시드 고정
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0,
                       X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)

plt.scatter(X_xor[y_xor == 1, 0],
            X_xor[y_xor == 1, 1],
            c='b', marker='x',
            label='1')
plt.scatter(X_xor[y_xor == -1, 0],
            X_xor[y_xor == -1, 1],
            c='r',
            marker='s',
            label='-1')

plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```



여기서 **np.random.randn()**은 기댓값이 0이고 표준편차가 1인 가우시안 표준 정규 분포를 따르는 난수를 생성한다.

![](https://user-images.githubusercontent.com/37826820/69909694-7580e500-1442-11ea-91da-912b7108e2fb.JPG)

```python
svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor,
                      classifier=svm)

plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```

SVC에 있는 kernel은 RBF 즉 Gaussian kernel을 말한다.

