# 3.5 커널 SVM을 사용하여 비선형 문제풀기



#### 선형 분류 모델의 한계

![](https://tensorflowkorea.files.wordpress.com/2017/06/2-36.png?w=625)

##### 																				예시로 사용되는 데이터셋



![](https://tensorflowkorea.files.wordpress.com/2017/06/2-37.png?w=625)

##### 																		선형 SVM으로 만들어진 결정 경계

> 선형 모델은 XOR 데이터셋을 구분할 수 없다. 
>
> [이미지 출저](https://tensorflow.blog/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2-3-7-%EC%BB%A4%EB%84%90-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0/)



위 데이터를 보면 선형분류로는 학습하는것이 불가능 하다. 이러한 문제를 해결하기 위해 저차원 데이터를 고차원 공간으로 매핑하여 분류한다.



![](http://i.imgur.com/4i3ILsZ.png)

> 원공간(INPUT Space)의 데이터를 선형 분류가 가능하도록 고차원 공간(Feature Space)로 매핑한 뒤 두 범주를 분류하는 초평면을 찾는 방법.



저차원 데이터를 고차원 데이터로 바꿔서 분류하는 것은 매우 효과적이지만 단점도 있다. 바로 **연산비용**이 매우 커진다는 점이다.  왜냐하면 모든 관측치에 대해 고차원으로 매핑한 뒤 이를 다시 **내적(inner product)** 해야하기 때문이다. 이러한 문제를 해결하기 위해 매핑과 내적을 한방에 할수 있는 **커널함수(Kernel function)** 를 사용한다.



## Kernel function

> 선형 분류가 어려운 저차원 데이터를 고차원 공간으로 매핑하는 함수.
> 두 폰인 사이 점곱을 계산하는 데 드는 높은 비용을 절감하기 위해 사용한다.

$$
\kappa(x^{(i)},x^{(j)}) = \varPhi(x^{(i)})^T\varPhi(x^{(j)})
$$

이러한 과정으로 **커널 K**의 값은 스칼라가 된다.



### 커널 사용의 장점

커널을 사용하면 basic 함수를 하나씩 정의하는 연산을 줄일 수 있어 계산량이 줄어든다. 예를 들어 다음과 같은 식으로 표현해 보겠다.
$$
\varPhi(x_i) = \varPhi([x_{i_1},x_{i_2}]) = (x^2_{i_1},\sqrt2x_{i_1}x_{i_2},x^2_{i_2})
$$
위 식에서 커널방법을 쓰지 않고 
$$
\varPhi(x_i)^T\varPhi(x_j)
$$
위의 결과를 구하기 위해서는
$$
\varPhi(x_i)^T: 곱하기 4번
$$

$$
\varPhi(x_j): 곱하기 4번
$$

$$
\varPhi(x_i)^T\varPhi(x_j): 내적 3번
$$



계산을 총 합하면 11번 하게 된다. 하지만 이 식에서 커널 방식을 쓰게 되면 연산이 확연하게 줄어드는것을 볼 수 있다.
$$
\begin{alignat}{2}
k(x_1,x_2) = (x^T_1x_2)^2
\\& = (x_{11}x_{21}+x_{12}x_{22})^2
\\& = x^2_{11}x^2_{21}+2x_{11}x_{21}x_{12}x_{22}+x^2_{12}y^2_{22}
\\& =(x^2_{11},\sqrt2x_{11}x_{12}x^2_{12})(x^2_{21},\sqrt2x_{21}x_{22},x^2_{22})
\\& = \varPhi(x1)^T\varPhi(x2)
\end{alignat}
$$


**Kernel** 방식을 사용하면 위 식을 계산하는데 곱셈이 2번 밖에 필요하지 않아 연산량이 크게 줄어든 것을 볼  수 있다.



- ##### Kernel의 종류(참고)

$$
\begin{align*}
linear\quad &:\quad K({ x }_{ 1 },{ x }_{ 2 })={ x }_{ 1 }^{ T }{ x }_{ 2 }\\ polynomial\quad &:\quad K({ x }_{ 1 },{ x }_{ 2 })={ ({ x }_{ 1 }^{ T }{ x }_{ 2 }+c) }^{ d },\quad c>0\\ sigmoid\quad &:\quad K({ x }_{ 1 },{ x }_{ 2 })=\tanh { \left\{ a({ x }_{ 1 }^{ T }{ x }_{ 2 })+b \right\}  } ,\quad a,b\ge 0\\ gaussian\quad &:\quad K({ x }_{ 1 },{ x }_{ 2 })=exp\left\{ -\frac { { \left\| { x }_{ 1 }-{ x }_{ 2 } \right\|  }_{ 2 }^{ 2 } }{ 2{ \sigma  }^{ 2 } }  \right\} ,\quad \sigma \neq 0
\end{align*} %]]>
$$



여기서 가장 많이 쓰이는 것은 **Gaussian Kernel(RBF Kernel이라고도 부름)**이다.

여기서 분모 1로 생략.
$$
\begin{align*}\\ K({ x }_{ 1 },{ x }_{ 2 })&=exp\left\{ -{ ({ x }_{ 1 }-{ x }_{ 2 }) }^{ 2 } \right\} \\ &=exp(-{ x }_{ 1 })exp(-{ x }_{ 2 })exp(2{ x }_{ 1 }{ x }_{ 2 })\end{align*} %]]>
$$
테일러급수 정리에 의해 
$$
exp(2x_ix_2)
$$
는 다음과 같이 쓸 수 있다.
$$
exp(2{ x }_{ 1 }{ x }_{ 2 })=\sum _{ k=0 }^{ \infty  }{ \frac { { 2 }^{ k }{ x }_{ 1 }^{ k }{ x }_{ 2 }^{ k } }{ k! }  }
$$
Gaussian Kernel은 input space가 몇 차원이든 무한대 차원의 Feature Space로 매핑한다는 얘기이다.(흠)

> 참고 자료들
>
> [Kernel svm 참조](https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/)
>
> [Kernel 참조(쉽게 설명 잘 해주심) ](https://datascienceschool.net/view-notebook/69278a5de79449019ad1f51a614ef87c/)





## **커널 SVM을 사용하여 비선형 문제 풀기**

```python
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(1)			# 시드 고정
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0,
                       X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)

plt.scatter(X_xor[y_xor == 1, 0],
            X_xor[y_xor == 1, 1],
            c='b', marker='x',
            label='1')
plt.scatter(X_xor[y_xor == -1, 0],
            X_xor[y_xor == -1, 1],
            c='r',
            marker='s',
            label='-1')

plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```



여기서 **np.random.randn()**은 기댓값이 0이고 표준편차가 1인 가우시안 표준 정규 분포를 따르는 난수를 생성한다.

![](https://user-images.githubusercontent.com/37826820/69909694-7580e500-1442-11ea-91da-912b7108e2fb.JPG)

```python
svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor,
                      classifier=svm)

plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```

SVC에 있는 kernel은 RBF 즉 Gaussian kernel을 말한다.

