## 4.1 누란된 데이터 다루기

실제 애플리케이션에서는 여러 가지 이유로 하나 이상의 누락된 값들이 있는 경우가 매우 흔하다. 이런 누락된 값들은 학습에 매우 많은 영향을 주기에 전처리가 필요하다.



- #### 데이터 전처리를 하는 이유

  모든 데이터 분석 프로젝트에서 데이터 전처리 과정은 필수로 거쳐야 하는 과정이다.   왜냐하면 학습 결과에 직접적으로 큰 영향을 주기 때문이다. 

  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile26.uf.tistory.com%2Fimage%2F99EBBA4A5B85404F27B2E9)

실무에 사용되는 데이터는 대부분 바로 사용할 수 없을 정도로 지저분하기에 전 처리 과정은 필수이다. 한 설문조사에서는 분석의 80% 시간을 데이터 수집 및 전처리에 사용 한다고 한다. 그만큼 학습을 시키기 위해서 가장 중요한 과정이라고 할 수 있다.

> 참고: [데이터 분석, 데이터 전처리에 대해(글쓰는 엔지니어 블로그)](https://writeren.tistory.com/99)
>
> 이미지 출저: https://writeren.tistory.com/99



### 4.1.1 테이블 형태 데이터에서 누락된 값 식별

```python
import pandas as pd
from io import StringIO

csv_data = \
'''A,B,C,D
1.0,2.0,3.0,4.0
5.0,6.0,,8.0
10.0,11.0,12.0,'''

df = pd.read_csv(StringIO(csv_data))
df
```

실행결과

> |    A |    B |    C |    D |      |
> | ---: | ---: | ---: | ---: | ---: |
> |    0 |  1.0 |  2.0 |  3.0 | 4.0  |
> |    1 |  5.0 |  6.0 |  NaN | 8.0  |
> |    2 | 10.0 | 11.0 | 12.0 | NaN  |

여기서 두개의 누락된 값을 만들었다. read_csv() 함수 안에있는 StringIO는 데이터를 DataFrame으로 변활 할 수 있게 한다.



- #### DataFrame에 누락된 값을 찾는 함수 insnull()

  아주 큰 DataFrame일 경우 수동으로 누락된 값을 찾는것은 힘들기 때문에 누락된 값을 찾기 위해 insnull() 함수를 사용한다.

  ```python
  df.isnull()
  ```

  > *>*
  >
  > ​	A		B		C		D
  >
  > 0 False False False False
  >
  > 1 False Flase True Flase
  >
  > 2 Flase Flase Flase True

  ```
  df.insnull().sum()
  ```

  > *>*
  >
  > A    0 
  >
  > B    0 
  >
  > C    1 
  >
  > D    1 
  >
  > dtype: int64

  .sum()을 사용하면 행의 NaN값들의 숫자를 세준다.



### 4.1.2 누락된 값이 있는 샘플이나 특성 제외

​	누락된 값을 가장 쉽게 처리하는 방법은 누락된 값이 있는 행이나 열을 삭제하는 것이다.

- #### df의 DataFrame

  > |      |    A |    B |    C |    D |
  > | ---: | ---: | ---: | ---: | ---: |
  > |    0 |  1.0 |  2.0 |  3.0 |  4.0 |
  > |    1 |  5.0 |  6.0 |  NaN |  8.0 |
  > |    2 | 10.0 | 11.0 | 12.0 |  NaN |



- #### 누락된 값이 있는 행을 삭제하는 dropna(axis=0)

  ```python
  df.dropna(axis=0)
  ```

  > |      |    A |    B |    C |    D |
  > | ---: | ---: | ---: | ---: | ---: |
  > |    0 |  1.0 |  2.0 |  3.0 |  4.0 |



- #### 누락된 값이 있는 열을 삭제하는 dropna(axis=1)

  ```
  df.dropna(axis=1)
  ```

  > |      |    A |    B |
  > | ---: | ---: | ---: |
  > |    0 |  1.0 |  2.0 |
  > |    1 |  5.0 |  6.0 |
  > |    2 | 10.0 | 11.0 |



- #### 모든 열이 NaN일때만 행을 삭제하는 dropna(how='all')

  ```
  df.dropna(how='all')
  ```

  > |      |    A |    B |    C |    D |
  > | ---: | ---: | ---: | ---: | ---: |
  > |    0 |  1.0 |  2.0 |  3.0 |  4.0 |
  > |    1 |  5.0 |  6.0 |  NaN |  8.0 |
  > |    2 | 10.0 | 11.0 | 12.0 |  NaN |



- #### 실수 값이 4개보다 작은 행을 삭제하는 dropna(thresh=4)

```
df.dropna(thresh=4)
```

> |      |    A |    B |    C |    D |
> | ---: | ---: | ---: | ---: | ---: |
> |    0 |  1.0 |  2.0 |  3.0 |  4.0 |



- #### 특정 열에 NaN이 있는 행만 삭제하는 dropna(subset=['C'])

```
df.dropna(subset=['C'])
```

> |      |    A |    B |    C |    D |
> | ---: | ---: | ---: | ---: | ---: |
> |    0 |  1.0 |  2.0 |  3.0 |  4.0 |
> |    2 | 10.0 | 11.0 | 12.0 |  NaN |



### 4.1.3 누락된 값 대체

샘플을 삭제하거나 특성 열이나 행을 삭제하는 것이 어려울 때가 있다. 유용한 데이터를 많이 잃을 수도 있기 때문이다. 이런 경우에는 **보간 기법**을 사용하여 데이터셋에 있는 다른 샘플로 부터 누락된 값을 추정할 수 있다.

> 기존의 함수는 0.22버전에서 삭제될 예정이므로 새롭게 추가된 것을 사용한다.

```python
import numpy as np
from sklearn.impute import SimpleImputer

simr = SimpleImputer(missing_values=np.nan, strategy='mean')
simr = simr.fit(df.values)
imputed_data = simr.transform(df.values)
imputed_data
```

기존의 것 과 달라진 것은 missing_values 에서 'NaN'으로 지정했던것을 np.nan으로 지정한다. missing_values의 기본 값은 np.nan으로 되어있다. strategy의 종류들로는 mean,median,most_frequent 외에 constant가 추가 되었다. constant는 **fill_value**에 미리 지정한 값을 NaN 자리에 넣는다.



##### constant 사용법

```
simr = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value= 5)	
simr = simr.fit(df.values)

constant_data = simr.transform(df.values)
```

> *>*
>
> [[ 1.  2.  3.  4.] 
>
> [ 5.  6.  5.  8.] 
>
> [10. 11. 12.  5.]]

constant를 제외한 나머지는 fiill_value 생략가능하다. 결과를 보면 미리 정해놓은 fill_value = 5의 값이 NaN위치에 들어간 것을 볼 수 있다.

SimpleImputer 함수는 axis의 기능이 없어 배열에서 열 계산을 수행할 수 없다. 그냥 기본이 axis = 0인 상태이다. 그래서 열의 값을 이용한 계산을 하고 싶다면 FunctionTransformer() 함수를 사용하면 된다.

##### FunctionTransformer() 함수

```python
from sklearn.preprocessing import FunctionTransformer
ftr_simr = FunctionTransformer(lambda X: simr.fit_transform(X.T).T, validate=False)
imputed_data = ftr_simr.fit_transform(df.values)
imputed_data
```

validata 를 True로 하면 2차원 배열로 변환한다. 불가능할 경우에는 오류가 뜬다.



## 4.2 범주형 데이터 다루기

실제 사용하는 데이터에는 수치형 데이터만이 아닌 하나 이상의 범주형 특성이 포함된 경우가 많다. 이런경우는 범주형을 수치형으로 바꿔서 사용하는 방법도 있다.



### 4.2.1 순서가 있는 특성과 순서가 없는 특성

범주형 데이터에는 순서가 있는 것을 구분해야한다.  예로 티셔츠 사이즈인 X,L M등은 순서를 알 수 있지만 색깔인 RED, BLUE 등은 어느것이 큰지 알 수 없다.



- #### 예제 데이터셋 만들기

  ```python
  import pandas as pd
  
  df = pd.DataFrame([['green', 'M', 10.1, 'class1'],
                     ['red', 'L', 13.5, 'class2'],
                     ['blue', 'XL', 15.3, 'class1']])
  
  df.columns = ['color', 'size', 'price', 'classlabel']
  df
  ```

  > |      | color | size | price | classlabel |
  > | :--: | :---: | :--: | :---: | :--------: |
  > |  0   | green |  M   | 10.1  |   class1   |
  > |  1   |  red  |  L   | 13.5  |   class2   |
  > |  2   | blue  |  XL  | 15.3  |   class1   |



### 4.2.2 순서 특성 매핑

학습 알고리즘이 순서 특성을 올바르게 인식하려면 범주형의 문자열 값을 정수로 바꿔야한다. 특히 특성을 자동으로 바꾸어주는 함수는 없을 수도 있기 때문에 매핑 함수를 직접 만들어야 한다.

```python
size_mapping = {'XL': 3,
                'L': 2,
                'M': 1}

df['size'] = df['size'].map(size_mapping)
df
```

size_mapping이라는 dictionary를 만든 후 df 배열의 'size'열을 .map() 함수로 사용하여 정수형으로 변환한다.

만약 나중에 정수값을 다시 원래 문자열 표현으로 바꾸고 싶다면 for문과 .items() 함수를 사용하면 된다.

```python
inv_class_mapping = {v: k for k, v in class_mapping.items()}
df['classlabel'] = df['classlabel'].map(inv_class_mapping)
df
```

> | color | color | size | price | classlabel |
> | ----: | ----: | ---: | ----: | ---------- |
> |     0 | green |    1 |  10.1 | 0          |
> |     1 |   red |    2 |  13.5 | 1          |
> |     2 |  blue |    3 |  15.3 | 0          |

.items() 함수를 사용하면 출력 결과는 ('XL' , '3'), ('L', '2') 이런식의 결과를 출력한다. 여기서 inv_class_mapping은 {3: 'XL', 2: 'L', 1: 'M'}의 아까와는 반대의 결과를 가제게 된다. 이것을 다시 .map() 함수를 이용하여 변환하면 처음과 같은 결과를 가질 수 있다.

##### class 레이블 인코딩 방법

```python
from sklearn.preprocessing import LabelEncoder

# 사이킷런의 LabelEncoder를 사용하여 레이블을 인코딩합니다.
class_le = LabelEncoder()
y = class_le.fit_transform(df['classlabel'].values)
y

#.inverse_transform() 함수를 사용해서 다시 원래대로 변환
class_le.inverse_transform(y)
```

> *>* array([0, 1, 0])	# 레이블을 정수로 인코딩
>
> *>* array(['class1', 'class2', 'class1'], dtype=object) #정수로 된 레이블을 다시 원래대로



#### 4.2.4 순서가 없는 특성에 원-핫 인코딩 적용

```python
X = df[['color', 'size', 'price']].values

color_le = LabelEncoder()
X[:, 0] = color_le.fit_transform(X[:, 0])
X
```

> array([[1, 1, 10.1],     
>
>   [2, 2, 13.5],
>
>    [0, 3, 15.3]], dtype=object)

이 방법은 컴퓨터가 green이 blue보다 크고 red는 green보다 크다고 가정해서 좋은 방법은 아니다.

이 문제를 해결하기 위해 **원-핫 인코딩** 기법을 사용한다



- #### 원-핫 인코딩

  > categorical_features는 0.22 버전에서 삭제될 예정

```python
oh_enc = OneHotEncoder(categories='auto')
col_trans = ColumnTransformer([('oh_enc', oh_enc, [0])], remainder='passthrough')
col_trans.fit_transform(X)
```

> *>*
>
> array([[0.0, 1.0, 0.0, 1, 10.1],     
>
>   		[0.0, 0.0, 1.0, 2, 13.5],     
>	
>   		[1.0, 0.0, 0.0, 3, 15.3]], dtype=object)

**원-핫 인코딩**으로 더미 변수를 만드는데 더 편리한 방법은 pandas의 get_dummies()함수를 사용하는 것이다. get_dummies()함수는 문자열 열만 변환하고 나머지 열은 그대로두는 함수이다.

```python
pd.get_dummies(df[['price', 'color', 'size']])
```

> |      | price | size | color_blue | color_green | color_red |
> | ---: | ----: | ---: | ---------: | ----------: | --------: |
> |    0 |  10.1 |    1 |          0 |           1 |         0 |
> |    1 |  13.5 |    2 |          0 |           0 |         1 |
> |    2 |  15.3 |    3 |          1 |           0 |     0원ㅎ |



- #### 원-핫 코딩의 문제점 다중 공성선(multicollinearity)

  - ##### 다중 공성선이란?

    - ##### 변수들이 서로 독립적인 관계가 아닌상호상관관계가 강한 상황.

    - ##### 회귀 계수의 분산을 증가시켜 불안정하고 해석하기 어렵게 만든다.

> [다중 공성선 내용](https://m.blog.naver.com/PostView.nhn?blogId=vnf3751&logNo=220833952857&proxyReferer=https%3A%2F%2Fwww.google.com%2F)

책에서는 다중 공선성 문제를 해결하기 위해 특성의 열 하나를 삭제한다.

```python
pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)
```

> | price | size | color_green | color_red |      |
> | ----: | ---: | ----------: | --------: | ---- |
> |     0 | 10.1 |           1 |         1 | 0    |
> |     1 | 13.5 |           2 |         0 | 1    |
> |     2 | 15.3 |           3 |         0 | 0    |

color_blue 열을 삭제해도 샘플에서 color_green =0 이고 color_red = 0 일때 color_blue가 1인것을 알기 때문에 잃는 정보는 없다.
