## 6.3 학습 곡선과 검증 곡선을 사용한 알고리즘 디버깅

 

### 6.3.1 학습 곡선으로 편향과 분산 문제 분석

---

 주어진 훈련 데이터셋에 비해 모델이 너무 복잡하거나 모델 파라미터가 너무 많으면 모델이  훈련 데이터에 과대 적합되는 경향이 있다. 이런 경우에는 보통 훈련 샘플을 더 모으거나 하면 문제를 해결할 수 있지만 실전상황에서는 데이터를 더 모으는것이 불가능한 경우가 대부분이다. 

 이런 상황에서는 모델의 훈련 정확도와 검증 정확도를 훈련 세트의 크기 함수로 그래프를 그려보면 모델에 높은 분산의 문제가 있는지 높은 편향의 문제가 있는지 쉽게 알 수 있다.  그래프를 그려보면 더 많은 데이터를 모으는게 문제를 해결할 수 있는지 판단이 가능하다.



- #### 편향과 분산

<img src="https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/3653/9376.png" style="zoom: 67%;" />

> [편향과 분산에대해서 잘 설명된 블로그](https://www.opentutorials.org/module/3653/22071)
>
> 이미지 출저: https://www.opentutorials.org/module/3653/22071

여기 그림은 편향과 분산을 설명하기 위한 예시를 가져온 그림이다.  

> - 빨간점: 정답
> - 파란점: 컴퓨터가 내놓은 예측값
> - 예측값들의 기대값(expectation) 즉 평균



<img src="https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/3653/9378.png" style="zoom:67%;" />

> 편향의 수식 표현
>
> 이미지 출저: https://www.opentutorials.org/module/3653/22071

 **편향(bias)**은 예측값과 정답이 얼마나 다른가를 의미하는 값이다.  즉 예측값과 정답을 빼면 서로 얼마나 떨어진지 알 수 있다는 의미이다.  

위의 수식을 보면 예측값들의 평균을 정답과 뺀 후 제곱을 해준것을 볼 수 있다. 제곱을 해 준 이유는 예측값들이 정답보다 큰경우도 있고 작은 경우도 있어 둘 사이의 거리값이 음수가 나오기도 하기 때문이다.  즉 제곱을 해주어서 양수로 만들어주기 위해 사용한 방법이다.



<img src="https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/3653/9379.png" style="zoom:67%;" />

> 분산의 수식 표현
>
> 이미지 출저: https://www.opentutorials.org/module/3653/22071

 **분산(variance)**은 서로 얼마나 흩어져 있는가에 대한 표현이다. 

위의 수식을 보면 예측된 값을 예측된 값의 평균에서 뺀 후 의 다시 평균을 내는 것을 알 수 있다. 이 식으로 서로 얼마나 흩어져 있는가를 표현할 수 있다.



#### 오차(Error)의 식

![](https://lh5.googleusercontent.com/eOxn3EwpqCNLuuj2UD2RTowh4mEYW1v9n08mJ7DQvlsn1j2plWaTtLZu-9qyIsbmzkwKAXrxKleut0z7_AiICJF3fFj4fjw-NxVUaPsTHPzWRPiA8aLu4as1z2BPB50NuTqg8WF3vII)

> 파란 점 하나(예측값)을 보고 표현한 식
>
> 맨 뒤의 기호는 근본적인 오차로 줄일 수 없는 오차이다. 
>
> 실제로 계산되는 값은 아님



- #### 편향-분산 트레이드 오프

![](https://user-images.githubusercontent.com/37826820/71761287-7bb82480-2f0c-11ea-9a3b-027c53cd0f7e.PNG)

> - 주황선: 훈련정확도
> - 파란선: 검증 정확도
> - 검은석 기대 정확도



 왼쪽 그림은 편향이 높은 모델을 보여준다.  그림을 보면 훈련 정확도와 교차 정확도가 모두 낮다는 것을 알 수 있다. 이것은 훈련 데이터에 과소적합이 되었다는 것을 의미한다. 이러한 문제를 해결하는 방법은 모델의 파라미터 개수를 늘리거나 SVM이나 로지스틱 회귀 분류기에서 규제 강도를 줄이면 된다.

 중간 그림은 분산이 높은  모델을 보여준다.  그림을 보면 훈련 정확도와 교차 검증 정확도 큰 차이가 있다는 것을 알 수 있다.  이런 경우는 모델이 훈련 데이터에 과대적합이 된 경우이다.  이 문제를 해결하려면 더 많은 데이터를 모으거나 모델 복잡도를 낮추면 된다.

오른쪽 그림은 가장 이상적인 모델을 보여준다. 편향은 낮으며 훈련 정확도와 교차 정확도가 서로 가까운 위치에 있어서 모델의 분산이 낮은 것을 알 수 있다.



- #### 모델평가

```python
from sklearn.model_selection import learning_curve


pipe_lr = make_pipeline(StandardScaler(),
                        LogisticRegression(solver='liblinear', 
                                           penalty='l2', 
                                           random_state=1))

train_sizes, train_scores, test_scores =\
                learning_curve(estimator=pipe_lr,
                               X=X_train,
                               y=y_train,
                               train_sizes=np.linspace(0.1, 1.0, 10),
                               cv=10,
                               n_jobs=1)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(train_sizes, train_mean,
         color='blue', marker='o',
         markersize=5, label='training accuracy')

plt.fill_between(train_sizes,
                 train_mean + train_std,	
                 train_mean - train_std,
                 alpha=0.15, color='blue')

plt.plot(train_sizes, test_mean,
         color='green', linestyle='--',
         marker='s', markersize=5,
         label='validation accuracy')

plt.fill_between(train_sizes,
                 test_mean + test_std,
                 test_mean - test_std,
                 alpha=0.15, color='green')

plt.grid()
plt.xlabel('Number of training samples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.8, 1.03])
plt.tight_layout()
plt.show()
```

 이 코드에서 train_sizes는 학습할 데이터의 수를 지정하는 것이다. np.linspace() 함수를 사용하여 10개의 일정한 간격을 가진 훈련 데이터를 설정했다. 

 cv 매개변수는 k값 지정을 의미한다. 코드에서는 cv의 값이 10으로 k =10, 10-겹 교차 검증이 사용된다.  

 plt.fill_between()함수는 구역을 색으로 채우는 함수이다. 그래프에 평균 정확도의 표준 편차를 그려서 추정 분산을 나타내기 위해 사용된다.



- #### 학습곡선 그래프

<img src="https://user-images.githubusercontent.com/37826820/71765606-53dfb580-2f3a-11ea-9f69-8505aaf08759.PNG"  />



학습 곡선 그래프를 보면 데이터 샘프 수가  250개 이상일 시 훈련 세트와 검증세트에서 잘 작동하는 것을 볼 수 있다. 반면에 250개 이하일 시에는 훈련세트는 정확도가 높고 검증 데이터에서는 정확도가 떨어지는 모습을 볼 수 있다. 이는 훈련세트에 학습이 과대적합되어 일어나는 현상으로 볼 수 있다.





### 6.3.2 검증 곡선으로 과대적합과 과소적합 조사

---

​	검증 곡선은 과대적합과 과소적합 문제를 해결하여 모델 성능을 높일 수 있는 유용한 도구이다. 검증 곡선은 훈련 정확도와 테스트 정확도를 그리는 대신 모델 파라미터 값의 함수를 그린다.



- #### 검증 곡선

```python
from sklearn.model_selection import validation_curve


param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_scores, test_scores = validation_curve(
                estimator=pipe_lr, 
                X=X_train, 
                y=y_train, 
                param_name='logisticregression__C', 
                param_range=param_range,
                cv=10)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(param_range, train_mean, 
         color='blue', marker='o', 
         markersize=5, label='training accuracy')

plt.fill_between(param_range, train_mean + train_std,
                 train_mean - train_std, alpha=0.15,
                 color='blue')

plt.plot(param_range, test_mean, 
         color='green', linestyle='--', 
         marker='s', markersize=5, 
         label='validation accuracy')

plt.fill_between(param_range, 
                 test_mean + test_std,
                 test_mean - test_std, 
                 alpha=0.15, color='green')

plt.grid()
plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1.00])
plt.tight_layout()
plt.show()
```

이 코드를 실행하면 매개변수 C에 대한 검증 곡선 그래프를 얻게 된다. 위의 학습 곡선과 다른 점은 param_name 매개변수가 있다는 점이다. 

 param_nam에 logisticregression__C 를 넣은 것은 회긔의 C 값에 대한 검증 곡선을 그린다는 의미이다.



![](https://user-images.githubusercontent.com/37826820/71766932-0c145a80-2f49-11ea-8106-c496bd1acc34.PNG)



그래프를 보면 parameter C의 값이 줄이면 (규제 강도를 높이면) 과소접합이 되는 모습을 보이고 parmeter C의 값을 높이면 (규제 강도를 줄이면)  학습 데이터에 과대 적합이 되는 경향을 볼 수 있다.



### 6.4 그리드 서치를 사용한 머신 러닝 모델 세부 튜닝

---

 그리드 서치는 하이퍼파라미터 값에 대한 최적의 조합을 찾음으로써 모델 성능을 향상시키는 효과적인 방법이다.

#### 6.4.1 그리드 서치를 사용한 하이퍼파라미터 튜닝

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_svc = make_pipeline(StandardScaler(),
                         SVC(random_state=1))

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]

param_grid = [{'svc__C': param_range, 
               'svc__kernel': ['linear']},
              {'svc__C': param_range, 
               'svc__gamma': param_range, 
               'svc__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  cv=10,
                  n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score_)
print(gs.best_params_)
```

> *>*> 
>
> 결과
>
> 0.9846153846153847 	# gs.best_score_
>
> {'svc*__*C': 100.0, 'svc*__*gamma': 0.001, 'svc*__*kernel': 'rbf'}
>
> gs.best_params_을 출력하면 최적의 하이퍼 파라미터를 알 수 있다.

 이 코드에서 핵심은 param_grid와 GridSearchCV 이다. param_grid는 최적의 조합을 찾기 위한 조합을 dictionary 형태로 저장한다. 변수로 C, kernel의 종류 , gamma 값을 설정한 것을 알 수 있다. 

GridSearchCV 클래스는는 param_grid 매개변수에 param_grid값을 넣어준다. 클래스 객체에 `fit` 메서드를 호출하면 grid search를 사용하여 자동으로 복수개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾아준다. 

GridSearchCV 클래스에서 n_jobs의 역활은 병령 프로세싱의 여부를 결정하는 역활을 한다.  n_jobs의 값을 -1로 하면 모든 프로세서를 활용해서 계산한다.

 

```python
clf = gs.best_estimator_
clf.fit(X_train, y_train)
print('테스트 정확도: %.3f' % clf.score(X_test, y_test))
```

> *>>* 테스트 정확도: 0.974



> GridSearch는 최적의 매개변수 조합을 찾을 수 있는 도구이지만 모든 매개변수 조합을 평가하기 위해 계산비용이 매우 많이든다. 이 대안으로 random search가 있다. 구역을 설정하고 random하게 매개변수 조합을 뽑는 방법이다.
>
> 궁금하다면 참고자료 -> [Bayesian Optimization 개요: 딥러닝 모델의 효과적인 hyperparameter 탐색 방법론](http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html)



#### 6.4.2 중첩 교차 검증을 사용한 알고리즘

 그리드 서치와 k-겹 교차 검증을 함께 사용하면 머신 러닝 모델의 세부 튜닝을 하기에 좋다. 여러 종류의 머신 너링 알고리즘을 비교하려면 중첩 교차 검증(nested cross-validation) 방법이 권장된다. 이러한 방식은 계산 성능이 중요한 대용량 데이터셋에서 유용하다. 중첩 교차 검증의 폴드 개수를 고려하여 5x2 교차 검증이라고도 한다.

![](https://i.stack.imgur.com/vh1sZ.png)

> 이해를 위한 그림
>
> 출저:  https://weina.me/nested-cross-validation/



```python
gs = GridSearchCV(estimator=pipe_svc,
                  param_grid=param_grid,
                  scoring='accuracy',
                  cv=2)
scores = cross_val_score(gs, X_train, y_train, 
                         scoring='accuracy', cv=5)
print('CV 정확도: %.3f +/- %.3f' % (np.mean(scores),
                                      np.std(scores)))
```

코드는 안쪽 루프에서 검증을 수행하여 모델을 선택한다. 모델이 선택된 후 바깥쪽 루프로 향해 선택된 모델로 모델의성능을 평가한다. gs에 GridSearchCV 클래스가 대입되어 gs의 값을 활용해 k-겹 교차검증을 다시 한번 하는 코드이다.



### 6.5 여러 가지 성능 평가 지표

---

 주어진 모델이 적합한지  측정할 수 있는 다른 성능 지표도 여럿 있는데 바로 정밀도(precision), 재현율(recall), F1-점수이다.



#### 6.5.1 오차 행렬

 이진 분류에서 성능 지표로 잘 활용되는 **오차행렬(Confusion Matrix, 혼동행렬)** 은 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표이다. 어떠한 유형의 예측 오류가 발생하는지를 확인할 수 있다.

![](https://i.imgur.com/IUrBHiD.png)

> 이미지 출저: https://subinium.github.io/basic-of-Evaluation/



```python
from sklearn.metrics import confusion_matrix

pipe_svc.fit(X_train, y_train)
y_pred = pipe_svc.predict(X_test)
confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)
print(confmat)
```

> *>>*
>
> [[71 1]
>
> ​	[2 40]]

```python
fig, ax = plt.subplots(figsize=(2.5, 2.5))
ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat.shape[0]):
    for j in range(confmat.shape[1]):
        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')

plt.xlabel('Predicted label')
plt.ylabel('True label')

plt.tight_layout()
plt.show()
```

<img src="https://user-images.githubusercontent.com/37826820/71778175-bcd43580-2fed-11ea-8bc0-97035603e8c6.PNG"  />

위 코드를 이어서 실행하면 아래와 같은 그래프를 얻을 수 있다.

이 그래프의 의미는 위에 왼쪽은 음성인 사람을 의미한고 오른쪽은 음성인데 양성으로 잘못 분류된 숫자가 표시되었다. 아래에서 왼쪽은 양성인데 음성으로 잘못 분류된 수이고 오른쪽은 양성인 사람들의 숫자이다.



#### 6.5.2 분류 모델의 정밀도와 재현율 최적화

 예측 오차(ERR)와 정확도(ACC)는 모두 얼마나 많은 샘플을 잘못 분류했는지 일반적인 정보를 알수 있다.
$$
ERR = \frac{FP+FN}{FP+FN+TP+TN}
$$

> 예측 오차는 잘못된 예측의 합을 전체 예측 샘플 개수로 나눈 것 이다.


$$
ACC = \frac{TP+TN}{FP+FN+TP+TN}=1-ERR
$$

> 정확도는 옳은 예측 합을 전체 예측 샘플 개수로 나눈 것이다.



- ##### 진짜 양성 비율(True Positive Rate, TPR)과 거깃 양성 비율(False Positive Rate, FPR)

  

  
  $$
  FPR = \frac{FP}{N}= \frac{FP}{FP+TN}
  $$
  
  $$
  TPR = \frac{TP}{P}= \frac{TP}{FN+TP}
  $$
  

  > TPR과 FPR은 오차 행렬에서 행(실제 클래스)끼리 계산하기 때문에 클래스 비율에 영향을 받지 않는다.



- ##### 정확도(PRE)와 재현율(REC) 성능 지표

  > 정확도와 재현율의 성능 지표는 진짜 양성과 진짜 음성 샘플의 비율과 관련이 있다.

  
  $$
  PRE = \frac{TP}{TP+FP}
  $$

- > 정확도는 모델이 True라고 분류하는 것 중 실제 True인 비율.

$$
REC = TPR = \frac{TP}{P} = \frac{TP}{FN+TP}
$$

> REC는 실제 True인 것 중 모델이 True라고 분류한 비율.



 PRE와 REC의 주 목적은 TP를 높이는 것이다.  하지막 재현율은 FN, 정밀도는 FP를 낮추는 데 초점을 둔다는 차이점이 있다.

> [설명 정말 잘되있는 곳 참조](https://subinium.github.io/basic-of-Evaluation/)



- ##### F1 스코어

  > F1 Score는 정밀도와 재현율을 결합한 지표이다. 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. 



![](https://t1.daumcdn.net/cfile/tistory/993482335BE0641515)

 식을 보면 각 값들이 조화 평균으로 계산되는 것을 알 수 있다.

> [F1 스코어 조화 평균에 대하여](https://sumniya.tistory.com/26)



```python
from sklearn.metrics import precision_score, recall_score, f1_score

print('정밀도: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))
print('재현율: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))
print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))
```

 scikit-learn에 구현되어 있어 간단히 구할 수 있다.



- ##### GridSearchCV에서 정확도 대신 다른 지표 사용하기

```python
from sklearn.metrics import make_scorer

scorer = make_scorer(f1_score, pos_label=0)

c_gamma_range = [0.01, 0.1, 1.0, 10.0]

param_grid = [{'svc__C': c_gamma_range,
               'svc__kernel': ['linear']},
              {'svc__C': c_gamma_range,
               'svc__gamma': c_gamma_range,
               'svc__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc,
                  param_grid=param_grid,
                  scoring=scorer,
                  cv=10,
                  n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score_)
print(gs.best_params_)
```

make_scorer()을 사용하여 정확도가 아닌 다른 성능 지표를 사용할 수 있다. 이 코드에서는 정확도가 아닌 f1_score를 사용한다.



#### 6.5.3 ROC(Receiver Operating Characteristic) 곡선 그리기



- ##### ROC 그래프

​	ROC 그래프는 임계 값을 바꾸어 가며 계산된 **FPR**과 **TPR** 점수를 기반으로 분류 모델을 선택하는 도구이다. 완변학 분류기 그래프는 **TPR**이 1이고 **FPR**이 9인 왼쪽 위 구석에 위치한다.



<img src="https://user-images.githubusercontent.com/37826820/71780072-63760180-3001-11ea-96e2-6f9d276dcaca.PNG" style="zoom:80%;" />



<img src="https://images.slideplayer.com/32/9873965/slides/slide_18.jpg" style="zoom: 67%;" />

> 이미지 출저: https://subinium.github.io/basic-of-Evaluation/

y축의 TPR은 REC의 값과 같다(TPR = REC(재현율)라고 생각하면 된다).  가장 이상적인 수치는 TPR이 1이고, FPR은 0인 상태이다. 

가운데 직선이 그려진 그래프는 동전 던지기 확률 50% 의 분류로 그려진 ROC 곡선이다. ROC 곡선이 직선에 가까울 수록 성능이 떨어지는 것이고, 멀어질수록 성능이 뛰어난 것 이다.

- ##### AUC(Area Under Curve)

AUC는 그래프의 아래 면적값을 계산하는 것이다.  ROC는 그래프이기 때문에 명확한 수치로써 비교하기가 어렵기 때문에 AUC 면적 값을 이용한다. 최대 값은 1이며 1에 가까울 수록 좋은 모델이라고 생각하면 된다.



#### 6.5.4  다중 분류의 성능 지표

 위에서 언급한 성능 지표는 이진 분류에 대한 것이다. 사이킷런에서는 이런 평균 지표에 마이크로(micro)와 마크(marcro)로 평균 방식을 구현하여 **OvA**방식을 사용하는 다중 분류로 확장한다.

- ##### 정밀도의 마이크로 평균(k의 개의 클래스가 있는 경우)

$$
PRE_{mircro}= \frac{TP_1+.....TP_k}{TP_1+....TP_k+FP_1+...FP_k}
$$

> 마이크로 평균은 클래스별로 TP, TN, FP FN을 계산한다.
>
> 마이크로 평균은 각 샘플이나 예측에 동일한 가중치를 부여하고자 할 때 사용한다.



- ##### 정밀도의 마크로 평균

  $$
  PRE_{macro}=\frac{PRE_1+.....+PRE_k}{k}
  $$

  > 마크로 평균은 클래스별 정밀도의 평균이다.
  >
  > 마크로 평균은 모든 클래스에 동일한 

  

  > 참고자료
  >
  > [micro and macro](https://unlimitedpower.tistory.com/entry/IR-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C-%ED%8F%89%EA%B7%A0Micro-average-%EB%A7%A4%ED%81%AC%EB%A1%9C-%ED%8F%89%EA%B7%A0Macro-average-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)
  >
  > [참고](https://m.blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221574025920&proxyReferer=https%3A%2F%2Fwww.google.com%2F)



```python
pre_scorer = make_scorer(score_func=precision_score, 
                         pos_label=1, 
                         greater_is_better=True, 
                         average='micro')
```

sklearn.metrics 모듈 아래에 있는 측정 함수들을 average 매개변수에 평균 계산 방식을 지정 할 수 있다. 위의 예 코드에서는 average 매개변수에 평균을 micro 방식을 사용하도록 설정 하였다.



### 6.6 불균형한 클래스 다루기

---

 클래스 불균형은 실전 데이터를 다룰 때 매우 자주 나타나는 문제이다. 보통 한 개 또는 여러 개의 클래스 샘플이 데이터셋에 너무 많을 때이다.

이 책에서는 불균형 상태일때, 소수 클래스의 샘플을 다수 클래스의 샘플 만큼 늘리는데 사용할 수 있는 resample 함수를 사용한다.



``` python
X_imb = np.vstack((X[y == 0], X[y == 1][:40]))
y_imb = np.hstack((y[y == 0], y[y == 1][:40]))
```

위 코드에서는 np.vstack, np.hstack 함수를 사용하여 각 0,1 레이블 들을 합친다.

```python
y_pred = np.zeros(y_imb.shape[0])
np.mean(y_pred == y_imb) * 100
```

> *>>*
>
> 89.92443324937027

np.zeros를 사용하여 레이블이 0인 비율을 알아보기 위해 작성된 코드이다.

0인 레이블이 전체에서 89.9%인 것을 볼 수 있다.



```python
from sklearn.utils import resample

print('샘플링하기 전의 클래스 1의 샘플 개수:', X_imb[y_imb == 1].shape[0])

X_upsampled, y_upsampled = resample(X_imb[y_imb == 1],
                                    y_imb[y_imb == 1],
                                    replace=True,
                                    n_samples=X_imb[y_imb == 0].shape[0],
                                    random_state=123)

print('샘플링한 후의 클래스 1의 샘플 개수:', X_upsampled.shape[0])
print(y_upsampled.shape[0])
```

> *>>*
>
> 샘플링하기 전의 클래스 1의 샘플 개수: 40 
>
> 샘플링한 후의 클래스 1의 샘플 개수: 357

 

 위 코드에서는 resample 함수를 사용하여 초기 레이블이 1인 개수가 40이였던 값을 357개로 늘린 모습을 볼 수 있다.



```python
y_pred = np.zeros(y_bal.shape[0])
np.mean(y_pred == y_bal) * 100
```

> *>>*
>
> 50.0

결과를 검증하기 위해 아까와 같은 코드로 테스트를 진행했다. up 샘플링 한 결과 아까와 다른 결과로 레이블 값이 1인 숫자가 전체의 50% 비율이 된것을 볼 수 있다.

비슷하게 다수 클래스의 훈련 샘플을 삭제해 다운 샘플링(downsampling)할 수 있는데. resample 함수를 사용하여 이전예에서 클래스 레이블 1과 0을 서로 바꾸면 된다.



- ##### 다운 샘플링한 예시

```python
from sklearn.utils import resample

print('샘플링하기 전의 클래스 0의 샘플 개수:', X_imb[y_imb == 0].shape[0])

X_downsampled, y_downsampled = resample(X_imb[y_imb == 0],
                                    y_imb[y_imb == 0],
                                    replace=True,
                                    n_samples=X_imb[y_imb == 1].shape[0],
                                    random_state=123)

print('샘플링한 후의 클래스 1의 샘플 개수:', X_downsampled.shape[0])
print(y_downsampled.shape[0])
```



 다운 샘플링을 적용한 예시이다. 위의 업 샘플링의 코드에서 X_imb[y_imb == 1]이였던 것을 X_imb[y_imb == 0], y_imb[y_imb == 0]으로 바꾸었다.

또한 기존 n_samples=X_imb[y_imb == 0].shape[0] 이였던 것을 n_samples=X_imb[y_imb == 1].shape[0]로 바꾸었다.

> *>>*
>
> 샘플링하기 전의 클래스 0의 샘플 개수: 357 
>
> 샘플링한 후의 클래스 0의 샘플 개수: 40 

결과를 보면 원하는데로 레이블이 0인 숫자가 357 -> 40으로 다운 샘플링 된것을 볼 수 있다.