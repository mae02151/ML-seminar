# 10장 회귀 분석으로 연속적 타깃 변수 예측



### 10.1 선형 회귀

- #### 선형 회귀

  > - 선형회귀는 하나 이상의 특성과 타깃 변수 사이의 관계를 모델링한것
  > - 특성이 하나이면 **단변량 선형 회귀**(univariate linear regression)
  > - 특성이 하나 이상이면 **다변량 선형 회귀**(multivaraiate linear regression)



- #### 단변량 선형 회귀

$$
y = w_{0}+w_{1}x
$$

> 머신러닝에서는 w가 가중치이지만 선형 회귀에서 w는 기울기를 의미한다.
>
> - 특성이 하나인 경우를 뜻함
> - w0: y축의 절편
>
> - w1: 특성의 가중치



- #### 다변량 선형 회귀

$$
y = {W_1x_1 + W_2x_2 + ... W_nx_n + b} = \sum_{i=0}^{m}w_{i}x_{i}=w^{T}x
$$

> 다변량 선형 회귀는 특성이 하나 이상인 경우를 말한다.



<img src="https://mblogthumb-phinf.pstatic.net/MjAxNzExMTVfMzQg/MDAxNTEwNjczNjk1MDQ1.tqykcpy1KAuDneZ4RTJpD2DoWsVPFtFJ6oAoyZ8RRJsg.jwBYSMHVT6nFKw3RKAd7mYDygzXJWEbnH61UvZvEJqkg.PNG.anthouse28/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2017-11-15_%EC%98%A4%EC%A0%84_12.33.47.png?type=w800" style="zoom:70%;" />

> 이미지 출저: https://mblogthumb-phinf.pstatic.net

이런 직선을 **회귀 직선**이라고 한다. 

회귀 직선과 샘플 포인트 사이의 직선 거리를 **오프셋(offset)** or **잔차(residual)**이라고 한다.



### 데이터셋의 중요 특징 시각화

---

 **탐색적 데이터 분석**(Exploratory Data Analysis, EDA)는 머신러닝 모델을 훈련하기 전 첫번째로 수행되는 중요한 단계이다. EDA 그래픽 도구 중 간단한 기법을 알아보자.



- #### 특성 간의 상관관계를 한번에 시각화 하기

  - Seaborn Library 사용



```python
import matplotlib.pyplot as plt
import seaborn as sns

cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']

sns.pairplot(df[cols], height=2.5)
plt.tight_layout()
plt.show()
```

- ##### 결과창

![](https://user-images.githubusercontent.com/37826820/72664034-95915580-3a3c-11ea-8333-e51cf4a1f7e1.png)

 seabor 라이브러리 에서 .pairplot을 사용하면 각 특성간의 산점도 행렬을 한눈에 볼 수 있다. <u>너무 좋은 기능인것 같다</u>

위 그래프를 보면 RM 특성과 MEDV 특성 사이의 관계가 선형적인 관계라는 것을 알 수 있다!



### 상관관계 행렬을 사용한 분석

---



#### **상관관계 행렬(correlation matrix)**

> 각각의  특성 사이의 선형 의존성을 측정.
>
> 즉 쉽게 말하면 두 특성이 완벽하게 양의 선형적인 관계를 가질 수록 1에 가깝고,  특성이 완벽하게 음의 선형적인 관계를 가질 수록 -1에 가까워진다.
>
> - r = 1: 완벽한 선형적 양의 관계
> - r = -1: 완벽한 선형적 음의 관계


$$
r = \frac{\sum_{i=1}^{n}[(x^{(i)}-\mu_{x})(y^{i}-\mu_{y})]}{\sqrt{\sum_{i=1}^{n}(x^{i}-\mu_{x})^2}\sqrt{\sum_{i=1}^{n}(y^{i}-\mu_{y})^2}}=\frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}
$$

$$
\sigma_{x} \sigma_{y}: 표준편차(x,y),\\n 
' '\sigma_{xy}: x와 y 사이 공분산
$$

> 특성이 표준화되어 있으면 상관관계 행렬과 공분산 행렬은 같다.



- #### Heatmap 함수 이용해 상관관계 보기

```python
import numpy as np


cm = np.corrcoef(df[cols].values.T)
#sns.set(font_scale=1.5)
hm = sns.heatmap(cm,
                 cbar=True,
                 annot=True,
                 square=True,
                 fmt='.2f',
                 annot_kws={'size': 15},
                 yticklabels=cols,
                 xticklabels=cols)

plt.tight_layout()
plt.show()
```

![](https://user-images.githubusercontent.com/37826820/72666631-a7342680-3a57-11ea-8b3e-24631e7811bc.PNG)

**heatmap** 그래프를 보면 각 특성과의 상관관계를 볼 수 있다.

위의 사진과 비교하면 RM특성 과 MEDV특성 사이가 선형관계였는데 여기서도 서로의 특성 상관관계 점수가 1에 가깝게 되어있는 것을 볼 수 있다.



- #### 공분산, 상관관계 행렬 관련 필기

![](https://user-images.githubusercontent.com/37826820/72694921-0ee29280-3b7a-11ea-96cc-871e34471fda.jpg)



